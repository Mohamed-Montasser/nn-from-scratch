# NN-from-scratch

**Build Your Own Neural Network Library & Advanced Applications**
*MCT Program â€” CSE473s: Computational Intelligence (Fall 2025)*

---

## ğŸ“Œ Project Overview

This repository will host our implementation of a **neural network library from scratch** using only **Python and NumPy**.
The project will later include:

1. A modular NN library (layers, activations, loss functions, optimizer, model class).
2. A demo solving the **XOR problem** using our custom library.
3. An **autoencoder** for MNIST image reconstruction.
4. Using the trained encoder as a **feature extractor** for SVM classification.
5. A comparison against **TensorFlow/Keras** implementations.

---

## ğŸ“ Planned Repository Structure

```
nn-from-scratch/
â”‚â”€â”€ lib/               # Neural network core implementation
â”‚â”€â”€ notebooks/         # Jupyter notebook (gradient check, XOR, autoencoder, SVM, comparisons)
â”‚â”€â”€ report/            # Final PDF report
â””â”€â”€ README.md
```

---

## ğŸ‘¨â€ğŸ« Course Information

**Course:** CSE473s â€” Computational Intelligence
**Program:** MCT
**Semester:** Fall 2025
**Project:** Build Your Own Neural Network Library & Advanced Applications

---

## ğŸ‘¥ Team Members

| ID      | Name                      |
| ------- | ------------------------- |
| 2100543 | Mohamed Montasser         |
| 2100660 | Fatma Samy Ahmed          |
| 2101231 | Moaz Gamal Alsayed        |
| 2100961 | Mohamed Islam Salah Aldin |
| 2100820 | Zeyad Samer Lotfy         |

---

## ğŸš€ Project Goals

* Understand foundational NN mathematics (forward/backward propagation).
* Implement essential components:

  * Dense layer, activation functions (ReLU, Sigmoid, Tanh, Softmax)
  * Loss functions (MSE)
  * Optimizer (SGD)
  * Sequential/Network model
* Validate the library through unit tests & gradient checking.
* Build an autoencoder and evaluate reconstruction quality.
* Use latent features for SVM classification.
* Compare performance with TensorFlow/Keras.

---
